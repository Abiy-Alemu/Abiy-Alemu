# -*- coding: utf-8 -*-
"""Spam Detection Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nKb-NmPMyJjtaKqYr6JDbGgcmKNe3LVE
"""

!pip install -U scikit-learn pandas matplotlib joblib

"""# Email Spam Detection â€” Full ML Pipeline (Run in Google Colab)

# file /content/drive/MyDrive/Colab Notebooks/spam.csv (uploaded to Colab )

#1. Imports
"""

import os
import pandas as pd, numpy as np
import matplotlib.pyplot as plt
import joblib
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix
from scipy.stats import randint
import seaborn as sns
import plotly.express as px # Added this import
import warnings
warnings.filterwarnings("ignore")

"""#2. Load dataset"""

DATA_PATH = "/content/drive/MyDrive/Colab Notebooks/spam.csv"
df = pd.read_csv(DATA_PATH, encoding='latin-1', low_memory=False)
print("Columns:", df.columns.tolist())
display(df.head(8))

# If your dataset has exactly two columns (label, text) and no headers, adapt as needed:
# Here we assume the first column is label, second column is text:
df = df.iloc[:, :2]
df.columns = ['label','text']
df = df.dropna(subset=['label','text']).copy()

df.head()

df.shape

df.info()

plt.rcParams['figure.facecolor'] = 'white'
plt.rcParams['axes.facecolor'] = 'white'
fig, ax = plt.subplots(1, 2, figsize=(15, 4))
ax = ax.flatten()
value_counts = df['label'].value_counts()
labels = value_counts.index.tolist()
colors =["#6782a8", "#ab90a0" ]
# Donut Chart
wedges, texts, autotexts = ax[0].pie(
    value_counts, autopct='%1.1f%%',textprops={'size': 9, 'color': 'white','fontweight':'bold' }, colors=colors,
    wedgeprops=dict(width=0.35),  startangle=80,   pctdistance=0.85  )
# circle
centre_circle = plt.Circle((0, 0), 0.6, fc='white')
ax[0].add_artist(centre_circle)

# Count Plot
sns.countplot(data=df, y=df['label'], ax=ax[1], palette=colors, order=labels)
for i, v in enumerate(value_counts):
    ax[1].text(v + 1, i, str(v), color='black',fontsize=10, va='center')
sns.despine(left=True, bottom=True)
plt.yticks(fontsize=9,color='black')
ax[1].set_ylabel(None)
plt.xlabel("")
plt.xticks([])
fig.suptitle('Spam - Ham Distribution', fontsize=15)
plt.tight_layout(rect=[0, 0, 0.85, 1])
plt.show()

df.describe()

df['Length']=df['text'].apply(len)
display(df.head())

fig = px.histogram(df, x='Length', color='label', marginal='rug',
                   title='Histogram of Text Length by Category')
fig.update_layout(
    xaxis_title='Length',
    yaxis_title='Frequency',
    showlegend=True)

# 1. Calculate the length of each email
df['text_length'] = df['text'].apply(len)

# 2. Outlier Detection using IQR (Interquartile Range) or simple percentile:
# Find the 95th percentile length. Emails longer than this are considered outliers.
Q3 = df['text_length'].quantile(0.75)
IQR = df['text_length'].quantile(0.75) - df['text_length'].quantile(0.25)
upper_bound = Q3 + 1.5 * IQR # Standard IQR approach
# Alternatively, use a percentile threshold for robustness
length_threshold = df['text_length'].quantile(0.95)

# 3. Remove (or inspect) Outliers
df_cleaned = df[df['text_length'] < length_threshold].copy()

print(f"\nEmails removed as outliers (length > {length_threshold:.0f}): {len(df) - len(df_cleaned)}")
print(f"Cleaned Data Size: {len(df_cleaned)}")

# Drop the temporary length column
df_cleaned.drop(columns=['text_length'], inplace=True)

from google.colab import drive
drive.mount('/content/drive')

from sklearn.feature_extraction.text import TfidfVectorizer

# 1. Split the Cleaned Data
X = df_cleaned['text']
y = df_cleaned['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 2. Initialize and Fit the TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(
    stop_words='english',
    lowercase=True, # Explicitly setting common cleaning steps
    token_pattern=r'\b[a-z]{3,}\b' # Only tokens of 3 or more letters
)

X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

print("\n--- Feature Extraction (TF-IDF) on Cleaned Data ---")
print(f"Training Features Matrix Shape: {X_train_tfidf.shape}")

"""# 3. Quick inspection"""

print("Shape:", df.shape)
print("Sample labels:", df['label'].unique()[:10])
display(df.sample(5, random_state=42))

"""# 4. Map labels to binary (spam=1, not spam=0)"""

unique = list(map(str.lower, df['label'].unique()))
if 'spam' in unique or 'ham' in unique:
    df['label_num'] = df['label'].apply(lambda x: 1 if str(x).strip().lower()=='spam' else 0)
else:
    # fallback: numeric codes (most frequent -> 0)
    df['label_num'] = pd.Categorical(df['label']).codes
    # If codes are not 0/1, you can remap accordingly:
    # df['label_num'] = (df['label_num'] != df['label_num'].mode()[0]).astype(int)

print(df['label'].value_counts())
print(df['label_num'].value_counts())

"""# 5. EDA"""

df['msg_len'] = df['text'].astype(str).str.len()

plt.figure(figsize=(6,3))
df['label_num'].value_counts().sort_index().plot(kind='bar')
plt.xticks([0,1], ['not_spam (0)','spam (1)'])
plt.title("Class distribution")
plt.ylabel("Count")
plt.tight_layout()
plt.show()

plt.figure(figsize=(6,3))
plt.hist(df['msg_len'], bins=40)
plt.title("Message length distribution")
plt.xlabel("Message length (chars)")
plt.tight_layout()
plt.show()

# Show a few spam & ham examples
print("\nExamples (spam):")
display(df[df['label_num']==1].sample(3, random_state=1)['text'])
print("\nExamples (not spam):")
display(df[df['label_num']==0].sample(3, random_state=1)['text'])

"""# 6. Split data"""

X = df['text'].astype(str)
y = df['label_num'].astype(int)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print(f"Train: {len(X_train)}, Test: {len(X_test)}")

"""#7. Build baseline pipelines"""

pipelines = {
    'logreg': Pipeline([('tfidf', TfidfVectorizer(stop_words='english', max_df=0.9)), ('clf', LogisticRegression(max_iter=1000))]),
    'nb': Pipeline([('tfidf', TfidfVectorizer(stop_words='english', max_df=0.9)), ('clf', MultinomialNB())]),
    'rf': Pipeline([('tfidf', TfidfVectorizer(stop_words='english', max_df=0.9)), ('clf', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))])
}

results = {}
for name, pipe in pipelines.items():
    pipe.fit(X_train, y_train)
    preds = pipe.predict(X_test)
    probs = pipe.predict_proba(X_test)[:,1] if hasattr(pipe, "predict_proba") else None
    results[name] = {
        'accuracy': accuracy_score(y_test, preds),
        'precision': precision_score(y_test, preds, zero_division=0),
        'recall': recall_score(y_test, preds, zero_division=0),
        'f1': f1_score(y_test, preds, zero_division=0),
        'roc_auc': roc_auc_score(y_test, probs) if probs is not None else None
    }
    print(f"{name}: acc={results[name]['accuracy']:.4f} f1={results[name]['f1']:.4f}")

"""# 8. Evaluate best baseline"""

best_baseline = max(results.keys(), key=lambda k: results[k]['f1'])
print("\nBest baseline by F1:", best_baseline)
print(classification_report(y_test, pipelines[best_baseline].predict(X_test), zero_division=0))

# Plot confusion matrix
cm = confusion_matrix(y_test, pipelines[best_baseline].predict(X_test))
plt.figure(figsize=(4,3))
plt.imshow(cm, interpolation='nearest')
plt.title(f'Confusion Matrix - {best_baseline}')
plt.colorbar()
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.xticks([0,1], ['not_spam','spam'])
plt.yticks([0,1], ['not_spam','spam'])
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j,i,cm[i,j],ha='center',va='center',color='white')
plt.tight_layout()
plt.show()

"""#  9. Quick hyperparameter tuning (RandomizedSearchCV)"""

# NOTE: If running on a slow environment, keep n_iter small (4-8). On Colab you can increase n_iter.
rf_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')), ('clf', RandomForestClassifier(random_state=42, n_jobs=-1))])
param_dist = {
    'tfidf__ngram_range': [(1,1),(1,2)],
    'tfidf__max_df': [0.8, 0.9],
    'clf__n_estimators': [100,200],
    'clf__max_depth': [None, 20],
    'clf__max_features': ['auto','sqrt']
}
rs = RandomizedSearchCV(rf_pipeline, param_dist, n_iter=6, cv=3, scoring='f1', n_jobs=-1, random_state=42, verbose=1)
rs.fit(X_train, y_train)
print("Best params:", rs.best_params_, "Best CV F1:", rs.best_score_)

best_model = rs.best_estimator_
best_preds = best_model.predict(X_test)
best_probs = best_model.predict_proba(X_test)[:,1] if hasattr(best_model, "predict_proba") else None

print("\nTuned model test performance:")
print("Accuracy:", accuracy_score(y_test, best_preds))
print("Precision:", precision_score(y_test, best_preds, zero_division=0))
print("Recall:", recall_score(y_test, best_preds, zero_division=0))
print("F1:", f1_score(y_test, best_preds))
if best_probs is not None:
    print("ROC-AUC:", roc_auc_score(y_test, best_probs))

"""# 10. Save final model

First, let's create a `requirements.txt` file in your Colab environment. This file will list all the Python libraries that your Streamlit app (`app.py`) needs to run.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile requirements.txt
# streamlit
# joblib
# pandas

# Re-extract the correct y_test to match X_test which was used for best_preds
# This assumes X_test in the kernel is the one from hdVjQDRsumiB split (length 1115)
# and the original df and split parameters are unchanged.
_, _, _, y_test_re_extracted = train_test_split(df['text'].astype(str), df['label_num'].astype(int), test_size=0.2, random_state=42, stratify=df['label_num'].astype(int))

baseline_f1 = results[best_baseline]['f1']
tuned_f1 = f1_score(y_test_re_extracted, best_preds) # Use the corrected y_test
if tuned_f1 >= baseline_f1:
    final_model = best_model
    chosen = 'rf_tuned'
else:
    final_model = pipelines[best_baseline]
    chosen = best_baseline

save_path = "final_spam_model.joblib"   # in Colab this will be in /content/
joblib.dump(final_model, save_path)
print(f"Saved final model ({chosen}) to {save_path}")

get_ipython().system('pip install -r requirements.txt')

get_ipython().system('ls -l')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# import streamlit as st
# import joblib
# import pandas as pd
# 
# # Load the trained model
# # The model was saved as 'final_spam_model.joblib' in the Colab environment.
# # If you saved it to Google Drive, you would need to mount Drive and adjust the path.
# 
# try:
#     model = joblib.load('final_spam_model.joblib')
# except FileNotFoundError:
#     st.error("Error: 'final_spam_model.joblib' not found. Make sure the model was saved in the current directory.")
#     st.stop()
# 
# st.title('Spam Detector')
# st.write('Enter a message below to check if it is spam or ham.')
# 
# # Text input from user
# user_input = st.text_area('Enter your message here:', '')
# 
# if st.button('Predict'):
#     if user_input:
#         # Make prediction
#         prediction = model.predict([user_input])
#         prediction_proba = model.predict_proba([user_input])
# 
#         # Display result
#         if prediction[0] == 1: # Assuming 1 is spam and 0 is ham
#             st.error(f"This message is SPAM! (Confidence: {prediction_proba[0][1]*100:.2f}%)")
#         else:
#             st.success(f"This message is HAM. (Confidence: {prediction_proba[0][0]*100:.2f}%)")
#     else:
#         st.warning('Please enter a message to predict.')
# 
# # Optional: Display some model info
# st.sidebar.header("About the Model")
# st.sidebar.write("This is a RandomForestClassifier model trained for spam detection.")

"""# 11. Summary table

"""

summary = pd.DataFrame([{'model':k, **v} for k,v in results.items()])
new_row = pd.DataFrame([{'model':'rf_tuned','accuracy':accuracy_score(y_test, best_preds),'precision':precision_score(y_test, best_preds, zero_division=0),'recall':recall_score(y_test, best_preds, zero_division=0),'f1':f1_score(y_test, best_preds),'roc_auc':roc_auc_score(y_test, best_probs) if best_probs is not None else None}])
summary = pd.concat([summary, new_row], ignore_index=True)
display(summary)